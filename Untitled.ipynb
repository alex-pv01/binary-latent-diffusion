{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1f2533-8da1-4b3e-b08b-d27c74ccd89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbaa7b1c-a940-4a9b-ab15-a37e04aebb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.sigmoid(torch.rand(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5b9c3c-a9a8-4fe7-9774-4888d0db49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e367a8-3660-4e58-853a-b57de09e5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"beans\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce8a0caa-dc17-42ee-9d9f-bef279fea18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.JpegImagePlugin.JpegImageFile"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[0]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa6788fb-c9d2-42c3-a430-cef9e951d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19c7f2a-f528-400f-88fe-9aafa1ab4f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 500, 500])\n"
     ]
    }
   ],
   "source": [
    "images = dataset[:100]['image']\n",
    "\n",
    "trans = transforms.ToTensor()\n",
    "\n",
    "tensor_list = []\n",
    "\n",
    "for image in images:\n",
    "    tensor_image = trans(image).unsqueeze(0)\n",
    "    tensor_list.append(tensor_image)\n",
    "\n",
    "tensor = torch.cat(tensor_list, dim=0)\n",
    "\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "661a0d76-a5e8-4a08-bec9-cdb350774626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "849dad33-bbbd-46ba-b85d-49e22aac8220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034, 500, 500, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = np.array(load_dataset(\"beans\", split=\"train\")['image'])\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69699fdb-f246-4520-827b-954c7e26a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12e740a4-5eed-4255-aacb-bb71802128fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be6cb5e6-c1be-4a85-a4a5-09aa2141b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = modules.Encoder(ch = 128, \n",
    "                         out_ch = 3, \n",
    "                         num_res_blocks = 2,\n",
    "                         attn_resolutions = [64,16], \n",
    "                         ch_mult = (1,1,2,2,4),\n",
    "                         in_channels = 3,\n",
    "                         resolution = 128, \n",
    "                         z_channels = 256,\n",
    "                         double_z = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fd3b654-401a-4a31-87b2-0867e48c0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dataset[:100]['image']\n",
    "\n",
    "batch = images[:3]\n",
    "\n",
    "trans = transforms.ToTensor()\n",
    "\n",
    "tensor_list = []\n",
    "\n",
    "for image in batch:\n",
    "    img = image.copy()\n",
    "    img = img.resize((128, 128))\n",
    "    tensor_image = trans(img).unsqueeze(0)\n",
    "    tensor_list.append(tensor_image)\n",
    "\n",
    "tensor = torch.cat(tensor_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0b859c6-0a5e-40c6-84a8-355323a94c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2b224ea-6d75-46cb-ae0a-110b2dbc6a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "h = encoder.forward(tensor)\n",
    "\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a88eb77-a78a-47b7-93d8-16f702b5840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 8, 8) = 16384 dimensions.\n"
     ]
    }
   ],
   "source": [
    "decoder = modules.Decoder(ch = 128, \n",
    "                         out_ch = 3, \n",
    "                         num_res_blocks = 2,\n",
    "                         attn_resolutions = [16], \n",
    "                         ch_mult = (1,1,2,2,4),\n",
    "                         in_channels = 3,\n",
    "                         resolution = 128, \n",
    "                         z_channels = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d27a0928-dd0a-4c81-bac8-1153aa3eaaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = decoder.forward(h)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c40e5060-1a63-46ac-ba1c-ed04564a2ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 8, 8])\n",
      "tensor([[0.4642, 0.5652, 0.4632, 0.3883, 0.5763, 0.6027, 0.5385, 0.5997],\n",
      "        [0.4801, 0.4085, 0.5135, 0.4570, 0.5150, 0.4254, 0.5556, 0.5512],\n",
      "        [0.4872, 0.4930, 0.6449, 0.5065, 0.5885, 0.6396, 0.6525, 0.5020],\n",
      "        [0.4595, 0.5613, 0.5960, 0.4315, 0.5173, 0.5507, 0.5855, 0.5291],\n",
      "        [0.4381, 0.4467, 0.5577, 0.5134, 0.4582, 0.6124, 0.4951, 0.5992],\n",
      "        [0.4206, 0.4470, 0.4368, 0.5505, 0.5270, 0.5357, 0.5210, 0.5448],\n",
      "        [0.4866, 0.4921, 0.5079, 0.5000, 0.4594, 0.5143, 0.5519, 0.5333],\n",
      "        [0.4826, 0.4969, 0.5068, 0.4753, 0.4501, 0.4871, 0.4826, 0.5082]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sigma_h = torch.sigmoid(h)\n",
    "print(sigma_h.shape)\n",
    "print(sigma_h[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4ad7245-fd41-4cbc-a39c-94df7c7c9aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 8, 8])\n",
      "tensor([[1., 1., 0., 0., 0., 1., 0., 1.],\n",
      "        [1., 0., 1., 0., 1., 0., 0., 1.],\n",
      "        [1., 1., 1., 1., 0., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 1., 1., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1.],\n",
      "        [1., 1., 0., 1., 0., 1., 1., 1.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 1., 0., 1., 0., 1.]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "binary = torch.bernoulli(sigma_h)\n",
    "print(binary.shape)\n",
    "print(binary[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f349d66-5adf-40b3-9b14-c668f910bba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 8, 8])\n",
      "tensor([[1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000],\n",
      "        [1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "aux_binary = binary.detach() + sigma_h - sigma_h.detach()\n",
    "print(aux_binary.shape)\n",
    "print(aux_binary[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bb10334-f339-445b-b89e-d50bc7a51b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = decoder.forward(aux_binary)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b84dbd3-bace-436c-bf6b-6d036f9d7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = modules.BinaryQuantizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36190d3b-9d2f-4c5d-8b83-90ef8e165db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = quantizer.forward(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba054c8d-d20c-4c28-9ad4-d7a085be7abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to the first available GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    # If no GPU is available, use the CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "362f447b-1502-4b9a-8c69-8035bfc34c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 8, 8) = 16384 dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/alex/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = bvae.BVAEModel(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45888cc-c466-4625-8bc5-899ec76f6f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n",
      "Working with z of shape (1, 32, 16, 16) = 8192 dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/alex/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  1\n",
      "Epoch  2\n",
      "Epoch  3\n",
      "Epoch  4\n",
      "Epoch  5\n",
      "Epoch  6\n",
      "Epoch  7\n",
      "Epoch  8\n",
      "Epoch  9\n"
     ]
    }
   ],
   "source": [
    "import bvae\n",
    "bvae.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b1ff77-7113-4303-bd88-57405a260517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa1f2d-497b-455b-a389-abbbf2640905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
